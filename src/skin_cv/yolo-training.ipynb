{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 10531480,
     "sourceType": "datasetVersion",
     "datasetId": 6517251
    }
   ],
   "dockerImageVersionId": 30840,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Data Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import kagglehub\n\npath = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n\nprint(\"Path to dataset files:\", path)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!mkdir ./datasets",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!mv $path ./datasets/skin-cancer",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!mkdir ./datasets/skin-cancer/images \n!cp -r ./datasets/skin-cancer/HAM10000_images_part_1/* ./datasets/skin-cancer/images && cp -r ./datasets/skin-cancer/HAM10000_images_part_2/* ./datasets/skin-cancer/images\n!rm -r ./datasets/skin-cancer/HAM10000_images_part_1 ./datasets/skin-cancer/HAM10000_images_part_2 ./datasets/skin-cancer/ham10000_images_part_1 ./datasets/skin-cancer/ham10000_images_part_2",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "class HAM10000Preprocessor:\n",
    "    def __init__(self, dataset_path, output_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.output_path = output_path\n",
    "        self.image_path = os.path.join(dataset_path, 'images')\n",
    "        self.metadata_path = os.path.join(dataset_path, 'HAM10000_metadata.csv')\n",
    "        \n",
    "        # Create label mapping\n",
    "        self.label_map = {\n",
    "            'nv': 0,    # Melanocytic nevi\n",
    "            'mel': 1,   # Melanoma\n",
    "            'bkl': 2,   # Benign keratosis\n",
    "            'bcc': 3,   # Basal cell carcinoma\n",
    "            'akiec': 4, # Actinic keratoses\n",
    "            'vasc': 5,  # Vascular lesions\n",
    "            'df': 6     # Dermatofibroma\n",
    "        }\n",
    "\n",
    "    def create_directories(self):\n",
    "        \"\"\"Create necessary directories for YOLO format\"\"\"\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            for subdir in ['images', 'labels']:\n",
    "                os.makedirs(os.path.join(self.output_path, split, subdir), exist_ok=True)\n",
    "\n",
    "    def load_metadata(self):\n",
    "        \"\"\"Load and preprocess metadata\"\"\"\n",
    "        df = pd.read_csv(self.metadata_path)\n",
    "        df = self.preprocess_metadata(df)\n",
    "        return df\n",
    "\n",
    "    def sanitize_bbox(self, bbox):\n",
    "        \"\"\"Sanitize bounding box coordinates to ensure they are within [0, 1] range.\"\"\"\n",
    "        return [max(0, min(1, coord)) for coord in bbox]\n",
    "\n",
    "    def create_yolo_annotation(self, image_id, label_id):\n",
    "        try:\n",
    "            image_path = os.path.join(self.image_path, f'{image_id}.jpg')\n",
    "            if not os.path.exists(image_path):\n",
    "                image_path = os.path.join(self.image_path, f'{image_id}.png')\n",
    "            \n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                return None\n",
    "            \n",
    "            height, width = img.shape[:2]\n",
    "            \n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if not contours:\n",
    "                x_center, y_center = 0.5, 0.5\n",
    "                box_width, box_height = 0.8, 0.8\n",
    "            else:\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "                \n",
    "                x_center = (x + w/2) / width\n",
    "                y_center = (y + h/2) / height\n",
    "                box_width = w / width\n",
    "                box_height = h / height\n",
    "            \n",
    "            bbox = self.sanitize_bbox([x_center, y_center, box_width, box_height])\n",
    "            \n",
    "            return f\"{label_id} {bbox[0]:.6f} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_id}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def split_dataset(self, df):\n",
    "        \"\"\"Split dataset into train, validation, and test sets\"\"\"\n",
    "        # First split: 80% train, 20% temp\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, \n",
    "                                           stratify=df['label_id'])\n",
    "        \n",
    "        # Second split: 10% validation, 10% test (from temp)\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42,\n",
    "                                         stratify=temp_df['label_id'])\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def process_split(self, df, split_name):\n",
    "        \"\"\"Process a dataset split\"\"\"\n",
    "        images_dir = os.path.join(self.output_path, split_name, 'images')\n",
    "        labels_dir = os.path.join(self.output_path, split_name, 'labels')\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=f'Processing {split_name}'):\n",
    "            image_id = row['image_id']\n",
    "            label_id = row['label_id']\n",
    "            \n",
    "            # Copy image\n",
    "            src_path = os.path.join(self.image_path, f'{image_id}.jpg')\n",
    "            if not os.path.exists(src_path):\n",
    "                src_path = os.path.join(self.image_path, f'{image_id}.png')\n",
    "            \n",
    "            if os.path.exists(src_path):\n",
    "                dst_path = os.path.join(images_dir, f'{image_id}{os.path.splitext(src_path)[1]}')\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                \n",
    "                # Create and save annotation\n",
    "                yolo_annotation = self.create_yolo_annotation(image_id, label_id)\n",
    "                if yolo_annotation:\n",
    "                    with open(os.path.join(labels_dir, f'{image_id}.txt'), 'w') as f:\n",
    "                        f.write(yolo_annotation)\n",
    "\n",
    "    def create_data_yaml(self):\n",
    "        \"\"\"Create data.yaml file for YOLO\"\"\"\n",
    "        yaml_content = {\n",
    "            'path': self.output_path,\n",
    "            'train': 'train/images',\n",
    "            'val': 'val/images',\n",
    "            'test': 'test/images',\n",
    "            'nc': len(self.label_map),\n",
    "            'names': list(self.label_map.keys())\n",
    "        }\n",
    "        \n",
    "        import yaml\n",
    "        with open(os.path.join(self.output_path, 'data.yaml'), 'w') as f:\n",
    "            yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "\n",
    "    def preprocess_metadata(self, metadata):\n",
    "        # Filling na age\n",
    "        metadata.fillna({'age': metadata['age'].mean()}, inplace=True)\n",
    "        if metadata[\"age\"].dtype == 'float64':\n",
    "            metadata[\"age\"] = metadata[\"age\"].astype(int)\n",
    "        \n",
    "        # Removing duplicates\n",
    "        lesion_id_cnt = metadata['lesion_id'].value_counts()\n",
    "        def check_duplicates_lesions(id):\n",
    "            if lesion_id_cnt[id] > 1:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        metadata['dup_les'] = metadata['lesion_id'].map(check_duplicates_lesions)\n",
    "        metadata = metadata[metadata.dup_les == False]\n",
    "\n",
    "        # Augmentation\n",
    "        def data_aug(df, col_name, val_name, class_max):\n",
    "            ''' This function balances the classes by 2 using the overrepresentation of the minor class'''\n",
    "            given_cols = df[df[col_name].isin([val_name, class_max])]\n",
    "            class_counts = df[col_name].value_counts()\n",
    "            class_count_max = class_counts[class_max]\n",
    "            class_count_val = class_counts[val_name]\n",
    "            max_class = given_cols[given_cols[col_name]==class_max]\n",
    "            val_class = given_cols[given_cols[col_name]==val_name]\n",
    "            val_class_over = val_class.sample(class_count_max,replace= True)\n",
    "            df = df[df[col_name] != val_name]\n",
    "            df = pd.concat([df,val_class_over],axis=0)\n",
    "            return df\n",
    "        count = metadata['dx'].value_counts().sort_index()\n",
    "        class_max = count.idxmax()\n",
    "        for val_name in metadata.dx.unique():\n",
    "            if val_name != class_max:\n",
    "                metadata = data_aug(metadata, 'dx', val_name, class_max)\n",
    "\n",
    "        # Add numerical labels\n",
    "        metadata['label_id'] = metadata['dx'].map(self.label_map)\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def process_dataset(self):\n",
    "        \"\"\"Main processing function\"\"\"\n",
    "        print(\"Starting dataset preparation...\")\n",
    "\n",
    "        # Create directories\n",
    "        self.create_directories()\n",
    "\n",
    "        # Load metadata\n",
    "        df = self.load_metadata()\n",
    "        print(f\"Loaded {len(df)} images metadata\")\n",
    "        \n",
    "        # Split dataset\n",
    "        train_df, val_df, test_df = self.split_dataset(df)\n",
    "        print(f\"Split sizes: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "        \n",
    "        # Process each split\n",
    "        self.process_split(train_df, 'train')\n",
    "        self.process_split(val_df, 'val')\n",
    "        self.process_split(test_df, 'test')\n",
    "        \n",
    "        # Create data.yaml\n",
    "        self.create_data_yaml()\n",
    "        \n",
    "        print(\"Dataset preparation completed!\")\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Total images: {len(df)}\")\n",
    "        print(\"\\nClass distribution:\")\n",
    "        class_dist = df['dx'].value_counts()\n",
    "        for cls, count in class_dist.items():\n",
    "            print(f\"{cls}: {count} ({count/len(df)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "dataset = \"./datasets/skin-cancer/images\"\n",
    "yolo_dataset = \"./datasets/skin-cancer-yolo\"\n",
    "\n",
    "preprocessor = HAM10000Preprocessor(dataset, yolo_dataset)\n",
    "preprocessor.process_dataset()"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Training & Eval",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "PRETRAINED_PATH = \"yolo11n.pt\"\n",
    "EPOCHS=75\n",
    "\n",
    "model = YOLO(\"PRETRAINED_PATH\")\n",
    "\n",
    "results = model.train(\n",
    "    data=f\"{yolo_dataset}/data.yaml\", \n",
    "    epochs=EPOCHS\n",
    ")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-21T12:35:53.584659Z",
     "iopub.execute_input": "2025-01-21T12:35:53.584951Z",
     "iopub.status.idle": "2025-01-21T13:21:32.925416Z",
     "shell.execute_reply.started": "2025-01-21T12:35:53.584929Z",
     "shell.execute_reply": "2025-01-21T13:21:32.924540Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "BEST_PATH = \"./runs/detect/train3/weights/best.pt\"\n",
    "\n",
    "model = YOLO(BEST_PATH)\n",
    "metrics = model.val(data=f\"{yolo_dataset}/data.yaml\")\n",
    "print(\"mAP50-95:\\t\", metrics.box.map)\n",
    "print(\"mAP50:\\t\", metrics.box.map50)\n",
    "print(\"mAP75:\\t\", metrics.box.map75)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-01-21T13:43:30.097794Z",
     "iopub.execute_input": "2025-01-21T13:43:30.098087Z",
     "iopub.status.idle": "2025-01-21T13:43:47.313201Z",
     "shell.execute_reply.started": "2025-01-21T13:43:30.098064Z",
     "shell.execute_reply": "2025-01-21T13:43:47.312161Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
